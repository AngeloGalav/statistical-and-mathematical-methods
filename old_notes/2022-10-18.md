## Yesterday's recap...

$f: R^n \longrightarrow R$
$\nabla f: R^n \longrightarrow R^n$
$\nabla f(x) = (\dfrac{\partial t}{\partial x_1}, \dfrac{\partial t}{\partial x_2}, \dots, \dfrac{\partial t}{\partial x_n})^T$

We must apply the chain rule to calculate the gradient. 

The loss function is $L(\theta)$, this is the notation that we use for the LSP (Least Squared Problem). The Loss function is defined as
$$
L(\theta) = min||\Phi \theta - y||^2_2
$$
where $y \in R^n$, $\Phi \in R^{n \cdot d}$. $L(\theta) : R \longrightarrow R$.

So if I derive L in respet to $e$, 

The 2-norm of the function can be written as $e^T \cdot e$ (the vector times the vector itself).
$e$ is a linear function, $y$ is a constant so we can forget it. What we obtain from the derivation is 

As we saw, the Jacobian is exactly the matrix $\Phi \in R^{n\cdot m}$. So we can compute the gradient of the LSP function as:
$$
\nabla L (\theta) = \dfrac{\partial L}{\partial e} \cdot \dfrac{\partial e}{\partial \theta} = 2 e^T \Phi = 2(\Phi \theta - y)^T \cdot \Phi
=2(\theta^T \Phi^T - y^T)\cdot \Phi
= 2\theta^T\Phi^T \Phi - 2y^T\Phi
$$


You can simplify, and you obtain this relation:
$$
\nabla L(\theta) = 0 \iff  2\theta^T\Phi^T \Phi - 2y^T\Phi = 0
$$
Which we can simplify to:
$$
(\theta^T\Phi^T \Phi - y^T\Phi)^T = 0
$$
$$
(\Phi^T \Phi \theta - \Phi^T y)^T = 0
$$
This is called the __normal equation__.

## Automatic differentiation
For training a deep network, we use the __backpropagation method__, but we will not see that in this course. 

Just know that in a deep network, $y$ (the result) is computed as a many level function. The idea of backpropagation is that we start from the end of the network. 

The backpropagation algorithm is a special case of a general technique, called **Automatic differentiation**. It is not a [..], but it is numerical method and it is based on working with intermediate variables, so I that I compute in a backward direction. 

Tha autormatic differentiation can be applied forward or backward, and allows us to compute the gradient. 

So, through the chain rule, we obtain:
$$
\dfrac{dy}{dx} = \dfrac{dy}{db}\dfrac{db}{da}\dfrac{da}{dx}
$$
Which can be computed in two ways: 
- _reverse mode_ since the gradients are propagated backwardsm 
$$
\dfrac{dy}{dx} = (\dfrac{dy}{db}\dfrac{db}{da})\dfrac{da}{dx}
$$
- _forward mode_ where the data flows from left to right. 
$$
\dfrac{dy}{dx} = \dfrac{dy}{db}(\dfrac{db}{da}\dfrac{da}{dx})
$$


#### A curious chain rule example:
\[Example 5.14 of the [`mml-book`](https://virtuale.unibo.it/pluginfile.php/1421704/mod_resource/content/0/mml-book_ch5.pdf)\]
This is the computation of $f$, but to derive $f$ I must first derive the blocks by deriving $a$ in respect to $x$.

To compute the function, I must convert all the intermediate functions into intermediate variables. 

The graph shows my though process when deriving the function: I'm going backward, from $f$ I encounter $e$ and $d$, which I substitute with their real value. 

Why are we using this method to calculate the formula?
We use the blocks that we generally can't use if you're calculating the gradient normally. Also, applying the chain rule is very difficult with a similar formula. We can also reuse some blocks, and in the case of NN the structure is always very similar, so we can reuse the blocks. 


### Reformulating Automatic Differentiation

We can formalize the graph representation of the function into a mathematical function. 

Suppose we have some input variables $x_1, x_2, \dots, x_d$ which are the input variables to the function, while $x_{d+1}, \dots, x_{D-1}$ are the intermediate variables, and $x_D$ is the output variable. 

The computation graph can be expressed as follows:

Intermediate variables are a function of , where as $G$ is an elementary function. 
$$
For \ i = d+1, \dots, D:\ x_i = g_i (x_{Pa(x_i)})
$$

where $g_i(\cdot)$ are elementary functions and $x_{Pa(x_i)}$ are the parent nodes of the variable x_i. 

Given a function defined in this way, we can use the chain rule to compute the derivative of the function in a step-by-step fashion. 

Recall that $f = x_D$, so:
$$
\dfrac{\partial f}{\partial x_D} = 1
$$
For other variables, we apply the chain rule:

$$
\dfrac{\partial f}{\partial x_i} = \sum_{x_j : x_i \in Pa(x_j)} \dfrac{\partial f}{\partial x_j} \dfrac{\partial x_j}{\partial x_i}
= \sum_{x_j : x_i \in Pa(x_j)} \dfrac{\partial f}{\partial x_j} \dfrac{\partial g_j}{\partial x_i}

$$



# Probability & Statistics

Some terms:
- The __sample space__ represents all the possible outcomes that an event can result in, and is denoted by $\Omega$.
- An __event__ is a subset of the sample space ($A \subset \Omega$). 
- The __probability__ is a function $P: A \longrightarrow [0, 1]$, so it's a function which takes an event and returns a value between 0 and 1.

Suppose we toss two coins. The possible outcomes are:
$$ \Omega = \{TT, HH, HT, TH\}$$
And the probability of $A = {TT, HH}$ is:
$$
P(A) = \dfrac{\#(A)}{\#(\Omega)}
$$
The function $\#$ returns the cardinality of a set. 

A __random variable__ $X$ is used to remap the result of an experiment to a number. In this way we can mathematically deal with a result of an experiment. So we could see it as a function $X: \Omega \longrightarrow R$. 

Suppose we take an experiment, and we take $X$ as the number of Heads in the toss. 
Some examples could be:
$$
\begin{align}
X(HH) = 2\\ 
X(HT) = 1\\
X(TH) = 1\\
X(TT) = 0
\end{align}
$$

The __target__ $T$ represents the possible values of the random variable $X$, so 
$T=\{0,1,2\}$. 

If we have a __discrete random variable__, $T$ is _finite_ and _countable_ (which is an infinte set but we can find the previous or next element of the set).
Else, if we have a __continuos random variable__, $T$ is _infinite_ and _not countable_.