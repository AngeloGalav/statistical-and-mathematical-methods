# Lecture 30-9-2022
## Intro
You can find the resources for this lesson [here](https://virtuale.unibo.it/pluginfile.php/1390672/mod_resource/content/0/mml_ch4.pdf)


### Image compression - Eckart-Young thereom
`(Page 34 of the resource cited in the intro)`

Consider a matrix $A \in R ^{mxn}$ of rank $r$ and let  $B \in R ^{mxn}$ be a matrix of rank $k$. For any k 6 r with Ab (k) = Pk i=1 iuiv> i it holds that:
![[Screenshot_20220930_155029.png]]

$$A - \hat A(k) = \sum_{i=r+1}^n \sigma_i u_i v_i^T$$
If we increase the height of the matrix [..]

[..]
$u_i$ is a vector of our $m$, and $v_i^T$ is letter of our $n$ . 
Since k is the sum is the sum of $(1 + m + n)\cdot k$ so $k(1+m+n) < m \cdot n$ ,

Compression of the image, compression factor is . [..]

For the image , there's a computation in the book where ...

If we consider, for example, the rank 5 approximation, we obtain an image that is pretty similar to the original, and we only need 0.6% of the data of the original image to store it. 

[..]
We project the image into a plane using the formula:
$z = u_k \cdot x$ , where $u_k$ is the matrix obtained from the first $k$ columns of $u$. 

Since $u$ is a projection matrix, we project only the first 2 columns. Since the two columns [..] this means that i keep the most important informartion of my image, and this is the reason you can create the clusters. 

So you can distinguish 4 from 6 even if you dont use the ground truth. Thanks to the fact that you can capture by using SVD the most important information of the image, you can almost distinguish 4 from 6. 

LDA will improve the separation of the clusters of the numbers. 

## Linear least-squared problem
We have a linear system $Ax=b$  without a solution, and we want to find it.  

Consider $x \in R^m, b \in R^n$. Is the solution unique? Let's then consider $||Ax -b||$. We want to find the vector $x$ so that the distance is the mininum possible. 
So the problem comes down to:
$$ \mathop{min}_{x \in R^n} ||Ax - b||_2 \iff \mathop{min}_{x \in R^n} ||Ax - b||_2^2
$$
$Ax-b$ is called the residual. 

Does the LLP has a solution? Is it unique?
In this case, the LLP always has a solution. Lets compute this solution:

1. $r=n, m = min (m,n)$The problem has a unique solution.
2. In the second point, we have to prove that the problem has infinite solution, and these infinite solutions constitute a vector space of size $n - r$.

$A^TA$ symmetric positive definite (non singular). 

### Direct Method

Cholesky factorization. $A=LL^T, B=A^TA$ where L is a Low triangular.  No pivoting is necessary. [..]

The computational cost of this factorization $O(n^3 / 6)$.  $Bx=A^Tb \implies Bx=z$ .
When you do the factorization we have to solve in sequence these following triangular systems:
1. $Ly=z$
2. $L^Tx=y$

If the direct methods are too expensive, you can use the __iterative methods__, such as the _coniugate gradient method_.

If A has maximum rank, it is singular so the system cannot be solved. This is also proof that the system has a unique solution.

We can use this method in the case in which $r \le n$ . We can use the SVD decomposition.    
The solution $x^*$ is equal to
$$
x^*= \sum_{i=1}^{r} \dfrac{u_i^Tb}{\sigma_i} v_i
$$

$x^* \in R^n$ is one of the infinite solution of hte liniarly squared problem. But it is the solution with the **minimum length**. You can prove that this minimum is unique. In symbols it means:
$$
x^* = 
$$
Pseudo-inverse of A, so A is not invertable A-1 doesnt not exists, but $A^+=(A^TA)^{-1}A$  in case that the rank is $m$. 
A^+ has some properties, such as 
1. $A^+A = I_n$ (the identity matrix over n)
2. $AA^+ = I_m$ (the identity matrix over m, so it's $m \cdot m$).
3. $n=m \implies A^+ = A^{-1}$

This way we can calculate $x^*$  as $x^* = A^+b$.  


## Compute data approximation (polinomial regression)
We want to compute a model (in this case a polynomial) which approximates the date usign a line. 
Here's an example:
![[outputpoly-1.png]]

The formula which obtains these is:
$f(x) = c_n\cdot x^n + c_{n-1}\cdot x^{n-1} + ... + c_1x +c_0$
I need to find the coefficent of this formula to solve the problem. So we need to find a way to compute these coefficients. 

In order to make model as close as possibile to the data.
To understand [..], we need to consider the distance (from an algebraic point of view) from the model to the points of the data.

The model should be adherent to the data as much as possible, so the distances must be as small as possible. 

How can you write these distances tho?
Considering $1 \le i \le n$ the formula for each point is $y_i - f(x_i) = y_i -Ax_i$  
Where $f$ is the model, which depends on the vector $\hat c = (c_0, c_1, ..., c_n)$ .
So, our problem comes down to:
$$
\mathop{min}_{\hat c \in R^n} \sum_{i=1}^m|y - Ac_i|^2 = 
$$
$$
= \mathop{min}_{\hat c \in R^n} ||y - Ac_i||_2^2
$$
We can see 
$$A\hat c = \begin{pmatrix}  
f(x_0) \\  
f(x_1) \\
...\\
f(x_m)
\end{pmatrix}$$

So essentially, I am choosing the one line which makes minimum the distance. 
The 2-norm is the most common for this type of approximation. 
There are some models which also use the 1-norm.