### Bho

__Maximum likelihood estimation (MLE)__
Estimate the parameters of a distribution probability functions.
We are given $x_1, x_2, \dots, x_N$, like $p(x_i | \theta)$. We want to estimate the parameter $\theta$ of our distribution. 

We define a function called _likelihood_, defined as $$L_x(\theta) = +log(\prod^N_{i=1} p(x_i | \theta))$$
After defining this function, we define the minimum in respect to $\theta$ as
$$
\theta^* = \max_{\mathbf{\theta}} L_x(\theta)
$$
The distribution probability function represents the best way of defining our data. 
Computationally it is usually more stable to computing the minimum, instead of maximizing the likelihood, so it is better to compute $\theta^*$ as 
$$
\theta^* = \min_{\mathbf{\theta}} - L_x (\theta)
$$

If I choose [..]

Example:
$$
\begin{align}
p(x_i | \theta) \sim N(x_i | \theta) \\
\theta = (\mu, \sigma^2)\\
L(\mu, \sigma^2)

\end{align}
$$If we consider instead the poisson distribution, we would have to do something like this:
$$
\begin{align}
p(x_i | \theta) \sim Pois(x_i | \lambda) \\ 
p(x_i) = exp(-\lambda) \dfrac{\lambda^{x_i}}{x_i!}

L(\theta) = log(\prod_{i=1}^{N} exp(- \theta) fuck)
\end{align}

$$
$$
\begin{align}
(x_n, y_n) \ n = 1 \dots N \\
x_n \in R^D \\
y_n \ labels(targets) 
\end{align}
$$
The ground truth of my samples. $y_n \in R$ in case of regressions, while as $y_n \in N$ in the case of classifications.   

Probabilistic function.
The probability is related to the labels, so the observations $x$ has to be probabilistic. Where as we want to determine a deterministic function describing $y_n,  \ n =1 \dots N$. 

We suppose that the probability from the previous pictures [..].
Formally we write this

[..]

The first choice [..]
The second choice is the kind of model, so I need to decide my function $f$, but the parameters of $f$ are not described by a parameters. 
The parameters that I now have to estimate are not the ones that I have to minimize.  
We assume then that $f(x)=\theta^T \cdot x$. 
It is a probabilistic function because the parameters of $f$ are the parameters of a probabilistic function. 

[..]

For this 

Now in place of $p$. 
$$
\begin{align}
L(\theta) = - \sum^{N}_{n=1} log(p(y_n) | x_n, \theta) \\ 
= - \sum^{N}_{n=1} log N (y_n | x_n^T \cdot \theta, \sigma^2) \\
= - \sum^{N}_{n=1} log (\dfrac{1}{\sqrt{2\pi \sigma}} \ exp(\dfrac{-(y_n - x_n^T \sigma)^2}{2\sigma^2}) \\
= - \sum^{N}_{n=1} log (\dfrac{1}{\sqrt{2\pi \sigma}} - \sum^{N}_{n=1} \ (\dfrac{-(y_n - x_n^T \sigma)^2}{2\sigma^2}) \\
= - \sum^{N}_{n=1} log (\dfrac{1}{\sqrt{2\pi \sigma}} + \sum^{N}_{n=1} \ (\dfrac{(y_n - x_n^T \sigma)^2}{2\sigma^2})
\end{align} 
$$
$$
\begin{align}
\min_{\mathbf{\theta}} \dfrac{1}{2\sigma^2}c - \sum^{N}_{n=1} (y_n - x_n^T \sigma)^2 = \\
\min_{\mathbf{\theta}}  \sum^{N}_{n=1} (y_n - x_n^T \sigma)^2 = \\
\min_{\mathbf{\theta}}  \sum^{N}_{n=1} (y_n - x_n^T \sigma)^2 

\end{align}
$$

$$
p(\theta | x) = \dfrac{p(x|\theta)p(\theta)}{p(x)}
$$

For the _bayes theorem_, 
$$
p(x|\theta) \cdot p(\theta) = p(\theta|x) \cdot p(x) = \max p(x|\theta) \cdot p(\theta) = \max p(\theta | x) \cdot p(x) = \max p(\theta | x)
$$
Reduce overfitting is gained by multiplying the likelihood by a term that is not $\theta$. 

In particular, we assume that the 
$\phi : R^D \longrightarrow R^x$
$$
\begin{pmatrix}

\end{pmatrix}
$$

[..]
If we estimate that 
$$
f(x) = \sum^{}_{}
$$

