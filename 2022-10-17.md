# Challenges in gradient based optimizations

- [mml-book](https://virtuale.unibo.it/pluginfile.php/1421704/mod_resource/content/0/mml-book_ch5.pdf)
- [aiopt ch5](https://virtuale.unibo.it/pluginfile.php/1432701/mod_resource/content/0/aiopt_ch5.pdf)


In today's lesson will discuss the two main problems associated with gradient-based optimization.

## Local Optima and Flat Regions
\[Resoruces in `aiopt ch5`\]

[..]

If you apply the gradient descent method. 
If you use a [..] you compute this local minimum, but obviously you can't know the shape of the function in this local minimum. 

There are regions related to the starting guess in which if you chose [..]

If you consider the 

starting guess is chosen randomly so you can realistically chose the best one for you. 

The second graph is an example of a function which 


si defined this way and the plot of the function computes/plots a flat region. 

In this case, the problem is that in flat region the steps are very slow, in fact the speed of gradient descent depends on the [..] and the gradient descent is very slow. It may require many steps, and accelarate through (?) to make the method faster.

The 3rd problem is differential [..]
So we consider two different loss function:
- $L = x^2 + y^2$
- $L=x^2 + 4y^2$

 In the first case we a circle and in the second one we have an ellipsis. 
 Geometrically, the gradient is ortogal/tangent in contour curve in that point. 

[..]

In this case it means that it is only sufficient in one case of the minimum. The gradient direction is the [..]

It is clear that to have the center of the ellipsis you must perform many steps. 
You have two different methods for the [..] second derivative. 

If you consider the second derivatives for the second function, you'll see that the changes in the X direction in small, while the changes of the direction in Y are big. 

The first step, in a sense, it removes the effects of the second one.
An effect of this is, for example, that you have features with very different attributes. 
In some section is very small, and in others is quite large.

What can be done to mitigate this issue is to _normlize_ the dataset.


# Constrained Optimization and Lagrange Multipliers

\[Resoruces in `mml ch7`\]

The minimization problem, as we have seen, on the solving of the following problem: 
$min f(x)$

We can put multiple constraints on this problem. Each solution must satisfied all the constrain put on the function, and has to solve the problem (that is, $min f(x)$). 
$m$ is the number of constrain which can be put on the solution. 

### Lagrange function
[..] 

The langrange function is basically the function incread of a term.
This is a [..] minimization, so we can ..

There are different


# Der
\[Resources @ `mml-book ch5`\]
Onestamente non c'ho capito un cazzo di questi 5 minuti. 

# Differentiation Rules

$f:R^n \longrightarrow R$
$\nabla f:R^n \longrightarrow R^n$
$\nabla f(x) = R^n \longrightarrow R^n$


|   Name   | Rule |
|------|------------|
| Product rule   | $(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$       |
| Quotient rule  | $(\dfrac{f(x)}{g(x)})' = \dfrac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2}$       |
| ¬ym  | 0.5        |
| ¬y¬m | 0.45       |

### Differentiation of composed multivariable functions

##### Case 1
Consider the functions:
$f:R^2 \longrightarrow R$
$g:R \longrightarrow R^2$
The value of the gradient of the function is: 
$$
	\dfrac {d (f \circ g)}{dt} = \underbrace{ (\dfrac{\partial t}{\partial x_1}, \dfrac{\partial t}{\partial x_2})}_{\nabla f(x)}
\begin{pmatrix}  
\dfrac {\partial x_1}{\partial t} \\  
\dfrac {\partial x_2}{\partial t} \\
\end{pmatrix}
$$

##### Example:

1.
$f:R^2 \longrightarrow R$
$g:R \longrightarrow R^2$

$f(x_1, x_2) = x_1^2 + 2x_2$
$g(t) = (sin(t), cos(t))$
$f \circ g : R \longrightarrow R$
$f(g(t)) = sin^2(t) + 2cos(t)$
$\dfrac{d (f \circ g)}{dt} = 2sin(t) \cdot \dfrac{dsint(t)}{dt} + 2 \dfrac{dcos(t)}{dt} = 2sin(t)cos(t) - 2sin(t)$

##### Case 2
$f:R^2 \longrightarrow R$
$g:R^2 \longrightarrow R^2$

The derivative of the composition is now a gradient.
$$\nabla (f \circ g)(s,t) = 
	(\dfrac{\partial t}{\partial x_1}, \dfrac{\partial t}{\partial x_2})
\begin{pmatrix}  
\dfrac {\partial x_1}{\partial s}  \dfrac {\partial x_1}{\partial t}\\  
\dfrac {\partial x_2}{\partial s}
\dfrac {\partial x_2}{\partial t}
\\
\end{pmatrix}
=
( , )

$$
Consider this real world example

Example @ 5.56b figure of the `mml-book`.

$f: R^p \longrightarrow R^m$
$g: R^n \longrightarrow R^p$

$f \circ g = R^n \longrightarrow R^m$

$F(x) = \begin{pmatrix} \end{pmatrix}$

## Jacobian
$$
J = \begin{pmatrix}
\dfrac{\partial F_1}{\partial x_1} \dfrac{\partial F_1}{\partial x_2} \dots \dfrac{\partial F_1}{\partial x_n} \\
\dots  \\
	\dfrac{\partial F_m}{\partial x_1}
	\dfrac{\partial F_m}{\partial x_2}
	\dots
	\dfrac{\partial F_m}{\partial x_n}
\end{pmatrix}
$$

The Jacobian of a linear [..] is a transformation matrix.

## Gradient of a Least-Squares Loss in a Linear Model
\[Example 5.11 of `mml-book`\]

We are changing notation.
The function:
$$
min||Ax-b||^2_2
$$
is now
$$
min||y -\Phi \theta||
$$
$\theta \in R^n$ is a vector of parameters. 