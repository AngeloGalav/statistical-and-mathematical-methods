## Statistics pt. 2
Suppose that $X$ is a discrete random variable (RV).

define 
$$
\begin{align}
p : T \longrightarrow [0,1]
\\ p\underline{(x)} = P(X = x)
\end{align}
$$

where $T$ is the random variable.
We call $p$ the _probability mass function_. 

In this case if I associate a 

The PMF has some properties, such as:
1. $p(x) \ge 0$
2. $\sum_{x \in T} \ { p (x)} = 1$

The mean of $p$ is defined as: 
$\mu = \sum_{x \in T} \ x p(x)$

While the variance is defined as:
$\sigma^2  = \sum_{x \in T} \ {(x - \mu)}^2 p(x) \ \in R$

Standard deviation:
$\sigma = \sqrt{\sigma^2} \ \in R$

#### Poisson distribution
When you speak about a distribution, you could speak about the PMF or another function (which will see later), it describes the probability of events. 

In this type of distribution, $\lambda$ represents the mean of the events in the unit time. 
In this case, the analitical representation of the PMF is
$$
\begin{align}
p(x) = e^{- \lambda} \dfrac{\lambda^x}{x!}, \\ x = 0,1,2...
\end{align}
$$
You can forget about this formula. 

The target of this random variable which is the poisson distributions, 

can be:
$\mu = \lambda, \sigma^2 = \lambda$

An example:
$$
\begin{align}
\lambda = 10
\\ p(x) = e^{-10} \dfrac{10^x}{x!} = P(X=x)
\\ p(12) = e^{-10} \dfrac{10^{12}}{12!}
\end{align}
$$

__Multivariate random variable__ consider a situation in which we have many random variables.
i.e., let's consider a bivariate RV:
$$
\begin{align}
x \rightarrow T_x
\\ y \rightarrow T_y
\end{align}
$$
What will we obtain through these type of RV, are __joint probability mass functions__, such as:
$$
\begin{align}
P_{x,y} = T_x \times T_y \longrightarrow [0, 1] 
\\ P_{x,y} = P(X=x, Y = y) 
\\ P_{xi, yi} = \dfrac{n_{ij}}{N} 
\end{align}
$$
N-total number of values. 

The marginal probabily is the probability distribution of each random variable. 
It is as if I consider only the probability of the value 4 for the RV $x$. 

The conditional probability of  X given Y:
$P(X = x_i | Y = y_i)  = \dfrac{n_{ij}}{r_{ij}}$
The marginal has also a factor $T$ which is not in this formula.

#### Marginal and Conditional probability
We have talked about marginal and conditional probability in this lesson, but we can generalize what we've said.

----------------
### Continuos RVs
We can also associate a fuctions to the RVs. They're called __Probability density functions__(PDF).
$$
\begin{align}
f: T \longrightarrow R
\\ T = [a, b]
\\ P(a \le x \le b) = \int_{a}^{b} f(x ) \,dx 
\end{align}
$$
$T$ is now a space, which can be seen as an interval or union of intervals.

If $x$ is a continuos RV, the probability that $P(X = x) = 0, \ \forall x$, since it cannot be equal to a precise value. 

Also, $f(x)$ is not a probability, and $f(x) \notin [0,1]$.

Again, we can define the mean of a PDF:
$$
\mu = \int_{x \in T}{x f(x) \ dx}
$$
The variance is:
$$
\sigma^2 = \int_{x \in T}{(x - \mu)^2f(x) \ dx}
$$
And the standard deviation remains $\sigma = \sqrt{\sigma^2}$.

-------------
### Gaussian (normal) distribution
The PDF of the gaussian distribution is:
$$
p(x) = \dfrac{1}{\sqrt{2 \pi \sigma^2}}exp(-\dfrac{(x-\mu)^2}{2\sigma^2})
$$
 ----------------
### Bayes theorem

[..]


- Product rule can be interpreted as the fact that every joint distribution of two random variables can be factorized.

The __bayes theorem__ states that:
$$
p(x|y) = \dfrac{p(y|x) \ p(x)}{p(y)}
$$
We want to make inference on the unknown variable $x$. We want to know the **posterior** (that is, the part on the left side of the equation). 
The prior is the marginal of x, which is unknown and so we dont know the prior.
We can suppose that $x$, for example, has a distribution [..] standard deviation and other crap. 

The unknown variable is also called __latent__, which is essentially what we want to know. (From now on, latent means something we want to know).

[..]

---------
### Statistic of a RV
A statistic of a RV is a deterministic function of that RV. 

__Summary statistics__ are statistics which give large information on RV.
__Expected value__ of a function $g : R \longrightarrow R$ of a univariate continuos RV $X \ p(X)$.

$$
E_x[g(x)] = \int_{x} g(x) \ p(x) dx \in R
$$
If we are considering a discrete RV, the intergral is substituted by a sum. 

If $X$ is multivariate, so it is in the form $X = [x_1, \dots , x_D]$, then
$$
E_x[g(x)] = \begin{pmatrix}
E_{x_1}[g(x)]
\\ \dots \\
E_{x_D}[g(x)]
\end{pmatrix}
\in R^D
$$

The __mean__ of a $X$ multivariate  is:
$$
E_x[x] = \begin{pmatrix}
E_{x_1}[x]
\\ \dots \\
E_{x_D}[x]
\end{pmatrix}
\in R^D
$$
$g(x) = x$. 

While, if we have a univariate variable:
$$
E_x[x] = \int_{}^{} x \ p(x) \ dx
$$
---------
## Covariance
Given $X,Y$ univariate. 
Covariance of X and Y:
$Cov[x, y] = E[x \cdot y] - E[x] \cdot E[y]$

The variance of $x$ is given by this formula: $V_x[x] = Cov[x,x]$.
The standard deviation of $x$ is $\sigma [x] = \sqrt{V[x]}$.
$$
V[x] = \int{}{}(x-\mu)^2 p(x) \ dx
$$
$$
$$