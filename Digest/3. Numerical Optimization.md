Machine learning models are expressed as _minimization problems_. In order to train such models one need to compute the best parameters for the training data set. 
The definition of best is modelled by the optimization of a given _objective function_, usually called __loss function__.

# Multivariate Functions

- Let $f : \mathbb{R}^n → \mathbb{R}$, then $f$ is _differentiable_ with respect to the parameter $x_i$, if the limit 
$$\lim_{h\to0} \dfrac{f(x_1,..., x_i +h, ..., x_n)}{h}= \dfrac{\partial f}{\partial x_i}(x)$$exists. 

- Let $f : \mathbb{R}^n → \mathbb{R}$ be a differentiable function. Then, the _grandient_ of $f$ is the vector is:
$$
\nabla f(x) = (\dfrac{\partial f}{\partial x_1}(x), ..., \dfrac{\partial f}{\partial x_n}(x))
$$

The basic rules of the gradients are the following:
- __Product rule__: 
$$
\dfrac{\partial}{\partial x}(f(x)g(x)) = \dfrac{\partial f}{\partial x}g(x) + f(x)\dfrac{\partial g}{\partial x}
$$
- __Sum rule__:
$$
\dfrac{\partial}{\partial x}(f(x) + g(x)) = \dfrac{\partial f}{\partial x} + \dfrac{\partial g}{\partial x}
$$
- __Chain rule__:
$$
\dfrac{\partial}{\partial x}(g \circ f)(x) = \dfrac{\partial}{\partial x}(g(f(x))) = \dfrac{\partial f}{\partial x}\dfrac{\partial g}{\partial x}
$$

Consider a function $f : \mathbb{R}^2 → \mathbb{R}$, with 2 variable $x_1$ and $x_2$ as parameters. 
Furthermore, $x_1(t)$ and $x_2(t)$ are themselves functions of $t$. To compute the gradient of $f$ w.r.t to $t$, we need to apply the chain rule for multivariate functions in this way:
$$
\dfrac{df}{dt} =


\begin{bmatrix}
    \dfrac{\partial f}{\partial x_1} & \dfrac{\partial f}{\partial x_2}
\end{bmatrix}
\begin{bmatrix}
    \dfrac{\partial x_1(t)}{\partial t} \\ \dfrac{\partial x_2(t)}{\partial t}
\end{bmatrix} = 
 \dfrac{\partial f}{\partial x_1}  \dfrac{\partial x_1}{\partial t}
+
   \dfrac{\partial f}{\partial x_2}  \dfrac{\partial x_2}{\partial t}
$$
where d denotes the gradient and $∂$ partial derivatives.

Example:
- Let's consider $f(x_1, x_2) = x_1^2 + 2x_2$, where $x_1 = \sin t$ and $x_2 = \cos t$. 
![[chain_rule_example.png]]

Another example:
![[example_gradient_2.png]]

## Jacobian
Considering a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ where $n \ge 1$ and $m \gt 1$, and a vector $x = [x_1, \dots, x_n]^T \in \mathbb{R}^n$, the corresponding vector of function values is given as: 
$$
f(x) = 
\begin{bmatrix}
f_1(x) \\ \vdots \\ f_m(x)
\end{bmatrix} 
\in \mathbb{R}^m
$$
$f$ is called a vector-valued function. Each element of $f$ is a function $f_i : \mathbb{R}^n \rightarrow \mathbb{R}$.
The _gradient_ of $f$ with respect to $x \in \mathbb{R}^n$ is:
$$
J = \nabla_xf = \dfrac{df(x)}{dx} =
\begin{bmatrix}
\dfrac{\partial f(x)}{\partial x_1} & \dots & \dfrac{\partial f(x)}{\partial x_n}
\end{bmatrix} = 
\begin{bmatrix}
   \dfrac{\partial f_1(x)}{\partial x_1} & \dots & \dfrac{\partial f_1(x)}{\partial x_n}
   \\ \vdots & & \vdots 
   \\ \dfrac{\partial f_m(x)}{\partial x_1} & \dots & \dfrac{\partial f_m(x)}{\partial x_n}
\end{bmatrix} 
\in \mathbb{R}^{m \times n} 
$$
$J$ is called the __Jacobian__ matrix. 

## Gradient Descent (mini-intro)
In many machines learning applications, one finds good model parameters by performing _gradient descent_, which relies on the fact that one can compute <u>the gradient of a learning objective</u> with respect to the parameters of the model. However, writing out the gradient is often impractical. 
In particular, considering a multilayer neural network, the output value can be computed through a multilevel function composition of the following form:
$$
y = (f_K \circ f_{K-1} \circ \dots \circ f_1)(x)
$$
where ==each layer can be seen as a function== that takes in input the previous layer’s output:
$$
f_i(x_{i-1}) = \sigma(A_{i-1} x_{i-1} + b_{i-1})
$$
\[continues towards describing the backpropagation method\]. 

## Automatic differentiation 
For training a deep network, we use the __backpropagation method__, but we will not see that in this course. The backpropagation algorithm is a special case of a technique called **Automatic differentiation**. 
This technique can be seen as a process ==to numerically compute the exact gradient of a function by working with intermediate variables and applying the chain rule==. 
Automatic differentiation has forward and reverse modes, depending on how gradients are propagated during the application of the chain rule. For example, considering the following graph:
![[automatic_diff_example.png]]
We can compute the gradient in two ways: 
- _reverse mode_ since the gradients are propagated backwards: 
$$
\dfrac{dy}{dx} = (\dfrac{dy}{db}\dfrac{db}{da})\dfrac{da}{dx}
$$
- _forward mode_ where the data flows from left to right:
$$
\dfrac{dy}{dx} = \dfrac{dy}{db}(\dfrac{db}{da}\dfrac{da}{dx})
$$

\[28\]
